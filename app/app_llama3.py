import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.prompts import PromptTemplate

# é¡µé¢è®¾ç½®
st.set_page_config(page_title="ğŸ§  RAG åŒ»ç–—é—®ç­”ç³»ç»Ÿ", layout="wide")
st.title("ğŸ§¬ æœ¬åœ° RAG + Llama3.1-8Bï¼ˆOllamaï¼‰ åŒ»ç–—é—®ç­”ç³»ç»Ÿ")

# åŠ è½½å‘é‡æ•°æ®åº“
@st.cache_resource
def load_vectorstore():
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
    return FAISS.load_local("D:\\Python_projects\\LLM\\MEDgpt-RAG\\data\\vect_medical_knowledge", embedding_model, allow_dangerous_deserialization=True)

vectorstore = load_vectorstore()
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Prompt æ¨¡æ¿
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ä½ æ˜¯ä¸€ä½åŒ»å­¦çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦åŠ å…¥ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·å›ç­”â€œæˆ‘æ— æ³•ç¡®å®šç­”æ¡ˆâ€ã€‚

ã€ä¸Šä¸‹æ–‡ã€‘
{context}

ã€é—®é¢˜ã€‘
{question}

ã€å›ç­”ã€‘

""".strip()
)

# LLM è®¾ç½®ï¼ˆå¯ç”¨æµå¼ï¼‰
llm = ChatOllama(model="llama3.1:8b", temperature=0.7, stream=True)

# ç”¨æˆ·è¾“å…¥
query = st.text_input("ğŸ’¬ è¯·è¾“å…¥ä½ çš„åŒ»å­¦é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯é«˜è¡€å‹çš„å¸¸è§å¹¶å‘ç—‡ï¼Ÿ")

if query:
    with st.spinner("ğŸ§  æ­£åœ¨åˆ†æå¹¶ç”Ÿæˆå›ç­”..."):

        # 1. æ£€ç´¢ä¸Šä¸‹æ–‡
        docs = retriever.get_relevant_documents(query)
        context = "\n\n".join([doc.page_content for doc in docs])

        # 2. æ„é€  prompt
        prompt = prompt_template.format(context=context, question=query)

        # 3. æµå¼è¾“å‡º
        st.markdown("### ğŸ“Œ å›ç­”ï¼š")
        output_placeholder = st.empty()
        full_response = ""

        for chunk in llm.stream(prompt):
            full_response += chunk.content
            output_placeholder.markdown(full_response)

        # 4. å±•ç¤ºåŸå§‹æ–‡æ¡£
        with st.expander("ğŸ“š æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£"):
            for i, doc in enumerate(docs):
                st.markdown(f"#### æ–‡æ¡£ {i+1}")
                st.code(doc.page_content[:1000], language='markdown')
